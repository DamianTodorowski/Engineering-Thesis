{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atak adwerialny na filtr spamu przy użyciu TextAttack\n",
    "\n",
    "Ten notatnik demonstruje, jak przeprowadzić atak zwodniczy na model wykrywania spamu przy użyciu biblioteki `textattack`. Użyjemy przepisu ataku `TextBugger` przeciwko filtrowi spamu opartemu na BERT-Tiny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zainstaluj niezbędne biblioteki\n",
    "#!pip install textattack transformers datasets tensorflow pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importowanie niezbędnych bibliotek\n",
    "import textattack\n",
    "import transformers\n",
    "from textattack.models.wrappers import HuggingFaceModelWrapper\n",
    "from textattack.datasets import HuggingFaceDataset\n",
    "from textattack.attack_recipes import TextFoolerJin2019, TextBuggerLi2018, DeepWordBugGao2018\n",
    "from textattack import Attacker\n",
    "from textattack.loggers import CSVLogger\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# textattack: Główna biblioteka do ataków adwersarzowych.\n",
    "# transformers: Biblioteka Hugging Face do modeli NLP.\n",
    "# HuggingFaceModelWrapper: Wrapper, który pozwala TextAttack korzystać z modeli Hugging Face.\n",
    "# HuggingFaceDataset: Klasa do ładowania zbiorów danych z Hugging Face.\n",
    "# TextFoolerJin2019, TextBuggerLi2018, DeepWordBugGao2018: Gotowe przepisy (recipes) ataków.\n",
    "# Attacker: Klasa zarządzająca procesem ataku.\n",
    "# CSVLogger: Do zapisywania wyników w pliku CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ładowanie modelu i tokenizera\n",
    "\n",
    "Użyjemy `mrm8488/bert-tiny-finetuned-sms-spam-detection`, modelu BERT-Tiny dostrojonego na zbiorze danych SMS Spam. Ten model jest stosunkowo mały i dosyć podatny na ataki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nazwa modelu do pobrania z Hugging Face Hub\n",
    "model_name = \"mrm8488/bert-tiny-finetuned-sms-spam-detection\"\n",
    "\n",
    "# Załadowanie modelu klasyfikacji sekwencji\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "# Załadowanie tokenizera odpowiadającego modelowi\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Opakowanie modelu i tokenizera w wrapper TextAttack, aby biblioteka mogła z nich korzystać\n",
    "model_wrapper = HuggingFaceModelWrapper(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Załadowanie zbioru danych\n",
    "\n",
    "Użyjemy zbioru danych `sms_spam` z biblioteki `datasets` Hugging Face. Uwaga: `sms_spam` zawiera tylko podział 'train', więc go użyjemy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Załaduj zbiór danych (sms_spam ma tylko podział 'train')\n",
    "# Musimy również określić kolumny zbioru danych, ponieważ textattack nie rozpoznaje automatycznie 'sms' jako kolumny wejściowej\n",
    "dataset = HuggingFaceDataset(\"sms_spam\", split=\"train\", shuffle=True, dataset_columns=(['sms'], 'label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Definicja Ataków Przeciwnych (Adversarial Attacks)\n",
    "\n",
    "W tej analizie skupimy się na trzech metodach generowania przeciwnych przykładów tekstowych. Użyjemy **TextBugger** (`TextBuggerLi2018`) jako głównego ataku, a **TextFooler** i **DeepWordBug** jako alternatyw porównawczych.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1. Główny Atak: TextBugger (`TextBuggerLi2018`)\n",
    "\n",
    "TextBugger jest **hybrydowym** atakiem typu **black-box**, zaprojektowanym do wprowadzania **minimalnych, zrozumiałych perturbacji** (zwanych \"bugami\") do tekstu. Jego celem jest spowodowanie błędnej klasyfikacji przez model docelowy przy jednoczesnym zachowaniu *utility* (zrozumiałości i użyteczności) dla ludzkiego oka.\n",
    "\n",
    "###  Mechanizm działania\n",
    "\n",
    "1.  **Identyfikacja Krytycznych Słów:** Atak najpierw wybiera słowa, które mają **największy wpływ** na pierwotną klasyfikację, mierząc spadek pewności modelu po ich usunięciu.\n",
    "2.  **Generowanie Perturbacji (Bugów):** Dla każdego krytycznego słowa generowane są kandydatury zmian na dwóch poziomach:\n",
    "\n",
    "| Poziom | Typ Perturbacji | Opis | Przykład (`love`) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Słowo (Word)** | Podstawianie (Sub-W) | Zastąpienie słowa **synonimem** lub semantycznie bliskim sąsiadem (z wektorów embeddings lub WordNet). | `love` $\\rightarrow$ `affection` |\n",
    "| **Znak (Character)** | Wstawianie (Insert-C) | Wstawienie losowej litery lub spacji wewnątrz słowa. | `love` $\\rightarrow$ `l ove` |\n",
    "| **Znak (Character)** | Zamiana (Swap-C) | Zamiana sąsiednich znaków wewnątrz słowa. | `love` $\\rightarrow$ `lvoe` |\n",
    "| **Znak (Character)** | Podstawianie (Sub-C) | Zastąpienie znaku wizualnie podobnym (np. **homoglify**) lub sąsiadem z klawiatury. | `love` $\\rightarrow$ `lovd` |\n",
    "\n",
    "3.  **Optymalizacja:** Wybierany jest kandydat, który **maksymalnie zmienia predykcję** na błędną klasę, jednocześnie minimalizując liczbę użytych modyfikacji.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2. Alternatywne Ataki Porównawcze\n",
    "\n",
    "Importujemy również `TextFooler` i `DeepWordBug` jako alternatywne strategie ataku, które różnią się metodologią perturbacji.\n",
    "\n",
    "###  TextFooler (`TextFoolerJin2020`)\n",
    "\n",
    "* **Charakterystyka:** Atak koncentrujący się wyłącznie na **podmianie słów** przy użyciu **synonimów**.\n",
    "* **Kluczowa Cecha:** Po wybraniu krytycznych słów, synonimy są filtrowane przez **Universal Sentence Encoder (USE)**, aby zapewnić, że nowy tekst ma to samo znaczenie (zachowanie wysokiego podobieństwa semantycznego). Jest to kluczowe dla zachowania płynności i sensu zdania.\n",
    "\n",
    "###  DeepWordBug (`DeepWordBugGao2018`)\n",
    "\n",
    "* **Charakterystyka:** Atak skupiający się wyłącznie na **perturbacjach na poziomie znaków** (literówki), wykorzystujący bardziej zaawansowany mechanizm wyboru celu.\n",
    "* **Kluczowa Cecha:** Jest to atak typu **white-box**, który wymaga dostępu do **gradientów** modelu. Używa techniki **Backward Differentiation** do obliczenia, które słowa są **najbardziej wrażliwe** na małe zmiany w swoich znakach, a następnie celuje w te znaki, aby maksymalnie zwiększyć stratę modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wybierz przepis ataku (attack recipe)\n",
    "attack_recipe = \"TextBugger\"\n",
    "\n",
    "if attack_recipe == \"TextBugger\":\n",
    "    # Zbuduj atak TextBugger dla naszego modelu\n",
    "    attack = TextBuggerLi2018.build(model_wrapper)\n",
    "elif attack_recipe == \"TextFooler\":\n",
    "    # Zbuduj atak TextFooler\n",
    "    attack = TextFoolerJin2019.build(model_wrapper)\n",
    "elif attack_recipe == \"DeepWordBug\":\n",
    "    # Zbuduj atak DeepWordBug\n",
    "    attack = DeepWordBugGao2018.build(model_wrapper)\n",
    "\n",
    "print(f\"Używany przepis ataku: {attack_recipe}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Uruchomienie ataku\n",
    "\n",
    "Uruchomimy atak na 10 przykładach, aby zademonstrować jego skuteczność. Zapiszemy również wyniki do wizualizacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atakuj 10 próbek\n",
    "# AttackArgs: argumenty dla ataku, takie jak liczba przykładów i logowanie do CSV\n",
    "attack_args = textattack.AttackArgs(num_examples=10, log_to_csv=\"results_spam.csv\")\n",
    "# Attacker: obiekt wykonujący atak na zbiorze danych\n",
    "attacker = Attacker(attack, dataset, attack_args)\n",
    "# Uruchomienie ataku i pobranie wyników\n",
    "results = attacker.attack_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Wizualizacja wyników\n",
    "\n",
    "### Podświetlone różnice w tekście\n",
    "Możemy wyświetlić oryginalny i zaburzony tekst z podświetlonymi różnicami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    # Wyświetl reprezentację HTML wyniku, pokazującą różnice\n",
    "    display(HTML(result.__str__(color_method='html')))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statystyki podsumowujące\n",
    "\n",
    "Spójrzmy na wskaźnik sukcesu i inne metryki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oblicz statystyki\n",
    "num_success = 0\n",
    "num_failed = 0\n",
    "num_skipped = 0\n",
    "\n",
    "for result in results:\n",
    "    # Sprawdź typ wyniku ataku\n",
    "    if isinstance(result, textattack.attack_results.SuccessfulAttackResult):\n",
    "        num_success += 1\n",
    "    elif isinstance(result, textattack.attack_results.FailedAttackResult):\n",
    "        num_failed += 1\n",
    "    elif isinstance(result, textattack.attack_results.SkippedAttackResult):\n",
    "        num_skipped += 1\n",
    "\n",
    "print(f\"Sukcesy: {num_success}\")\n",
    "print(f\"Porażki: {num_failed}\")\n",
    "print(f\"Pominięte: {num_skipped}\")\n",
    "\n",
    "# Rysowanie wykresu słupkowego\n",
    "labels = ['Sukces', 'Porażka', 'Pominięte']\n",
    "values = [num_success, num_failed, num_skipped]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(labels, values, color=['green', 'red', 'gray'])\n",
    "plt.title('Podsumowanie wyników ataku')\n",
    "plt.xlabel('Typ wyniku')\n",
    "plt.ylabel('Liczba')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretacja wyników\n",
    "\n",
    "Wykres słupkowy przedstawia podsumowanie skuteczności ataku adwersarzowego na wybranej próbce danych. Poniżej znajduje się wyjaśnienie poszczególnych kategorii:\n",
    "\n",
    "- **Sukces (Success)**: Liczba przykładów, dla których atak zakończył się powodzeniem. Oznacza to, że algorytm zdołał zmodyfikować tekst wejściowy w taki sposób, że model zmienił swoją predykcję (np. z \"spam\" na \"nie-spam\"), zachowując jednocześnie semantyczne podobieństwo do oryginału.\n",
    "\n",
    "- **Porażka (Failed)**: Liczba przykładów, dla których atak nie powiódł się. Algorytm nie był w stanie znaleźć takiej modyfikacji tekstu, która zmyliłaby model, przy zachowaniu zadanych ograniczeń (np. maksymalna liczba zmienionych słów, minimalne podobieństwo semantyczne).\n",
    "\n",
    "- **Pominięte (Skipped)**: Liczba przykładów, które zostały pominięte w procesie ataku. Najczęściej dzieje się tak, gdy model błędnie sklasyfikował oryginalny tekst jeszcze przed atakiem. Ataki adwersarzowe zazwyczaj przeprowadza się tylko na poprawnie sklasyfikowanych przykładach, aby mierzyć skuteczność samego ataku, a nie błędy modelu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
